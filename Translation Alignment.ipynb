{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Alignment using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment\n",
    "A storage class for representing alignment between two sequences, $s_1$, $s_2$. In general, an alignment is a set of tuples of the form $(i, j)$ representing an alignment between the $i$-th element of $s_1$ and the $j$-th element of $s_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import Alignment\n",
    "\n",
    "a = Alignment([(0, 0), (0, 1), (1, 2), (2, 2)])\n",
    "a.invert() # Reversing the directionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a giza-formatted string and return an Alignment object.\n",
    "Alignment.fromstring('0-0 2-1 9-2 21-3 10-4 7-5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlignedSent\n",
    "`AlignedSent` encapsulates two sentences along with an ``Alignment`` between them. Typically used in machine translation to represent a sentence and its translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.api import AlignedSent\n",
    "\n",
    "algnsent = AlignedSent(['klein', 'ist', 'das', 'Haus'],\n",
    "                       ['the', 'house', 'is', 'small'], \n",
    "                       Alignment.fromstring('0-3 1-2 2-0 3-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algnsent.words # Words in the target language sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algnsent.mots # Words in the source language sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algnsent.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAlignment(als):\n",
    "    print(\" \".join(als.words))\n",
    "    print(\" \".join(als.mots))\n",
    "    print(\"\\n\".join([als.words[a[0]]+\" == \"+als.mots[a[1]] for a in als.alignment \n",
    "                     if isinstance(a[0], int) and isinstance(a[1], int) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAlignment(algnsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ugarit import Ugarit\n",
    "ugarit = Ugarit()\n",
    "ugarit.render(algnsent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Aligned Corpus\n",
    "NLTK makes available as the comtrans corpus a subset of Europarl's sentence-aligned English/French, German/English and German/French data, approximately 33,000 sentence pairs in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import comtrans\n",
    "fileids = comtrans.fileids()\n",
    "fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in fileids:\n",
    "    print(id +\" \"+ str(len(comtrans.sents(id))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = comtrans.words('alignment-en-fr.txt')\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = comtrans.aligned_sents('alignment-en-fr.txt')[0]\n",
    "als.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAlignment(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugarit.render(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugarit.render(als.invert())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Models\n",
    "\n",
    "The IBM models are a series of generative models that learn lexical translation probabilities, \n",
    "\n",
    "`P(target language word|source language word)` ,\n",
    "\n",
    "given a sentence-aligned parallel corpus.\n",
    "\n",
    "The models increase in sophistication from model 1 to 5. Typically, the output of lower models is used to seed the higher models. \n",
    "\n",
    "All models use the <b>Expectation-Maximization (EM)</b> algorithm to learn various probability tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "we will use the Bible parallel corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# read the data from\n",
    "f = open(\"data/txt/English.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "english = {}\n",
    "# split\n",
    "for line in lines:\n",
    "    temp = line.split(\"\\t\") # b.GEN.1.1\tIn the beginning God created the heaven and the earth.\n",
    "    # indexing & tokenization\n",
    "    english[temp[0]] = tokenizer.tokenize(temp[1].strip().lower())\n",
    "english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/txt/German.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "german = {}\n",
    "for line in lines:\n",
    "    temp = line.split(\"\\t\") # b.GEN.1.1\tAm Anfang schuf Gott Himmel und Erde.\n",
    "    # indexing & tokenization\n",
    "    german[temp[0]] = tokenizer.tokenize(temp[1].strip().lower())\n",
    "german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two files in one parallel corpus i.e. a list of AlignedSent\n",
    "parallelSentences = [AlignedSent(s, english[id]) for id, s in german.items() if id in english]\n",
    "parallelSentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Model 1\n",
    "In IBM Model 1, word order is ignored for simplicity. As long as the word alignments are equivalent, it doesn’t matter where the word occurs in the source or target sentence. Thus, the following three alignments are equally likely.\n",
    "- Source: je mange du jambon\n",
    "- Target: i eat some ham\n",
    "- Alignment: (0,0) (1,1) (2,2) (3,3)\n",
    "\n",
    "\n",
    "- Source: je mange du jambon\n",
    "- Target: some ham eat i\n",
    "- Alignment: (0,2) (1,3) (2,1) (3,1)\n",
    "\n",
    "\n",
    "- Source: du jambon je mange\n",
    "- Target: eat i some ham\n",
    "- Alignment: (0,3) (1,2) (2,0) (3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    The EM algorithm used in Model 1 is:\n",
    "\n",
    "- <b>E step</b> - In the training data, count how many times a source language word is translated into a target language word, weighted by the prior probability of the translation.\n",
    "\n",
    "- <b>M step</b> - Estimate the new probability of translation based on the counts from the Expectation step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import IBMModel1\n",
    "em_ibm1 = IBMModel1(parallelSentences[:300], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = em_ibm1.translation_table['geist']\n",
    "sorted(d.items(), key=lambda x: x[1],reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = em_ibm1.translation_table['wasser']\n",
    "sorted(d.items(), key=lambda x: x[1],reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = em_ibm1.translation_table['gott']\n",
    "sorted(d.items(), key=lambda x: x[1],reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAlignment(parallelSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugarit.render(parallelSentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Model 2\n",
    "Lexical translation model that considers word order. IBM Model 2 improves on Model 1 by accounting for word order. An alignment probability is introduced, a(i | j,l,m), which predicts a source word position, given its aligned target word’s position.\n",
    "\n",
    "Notations:\n",
    "- $i$: Position in the source sentence \n",
    "    (Valid values are 0 (for NULL), 1, 2, ..., length of source sentence)\n",
    "- $j$: Position in the target sentence \n",
    "    (Valid values are 1, 2, ..., length of target sentence)\n",
    "- $l$: Number of words in the source sentence, excluding NULL\n",
    "- $m$: Number of words in the target sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    The EM algorithm used in Model 2 is:\n",
    "\n",
    "- <b>E step</b> - In the training data, collect counts, weighted by prior probabilities.\n",
    "\n",
    "(a) count how many times a source language word is translated into a target language word\n",
    "\n",
    "(b) count how many times a particular position in the source sentence is aligned to a particular position in the target sentence\n",
    "\n",
    "- <b>M step</b> - Estimate new probabilities based on the counts from the E step\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import IBMModel2\n",
    "em_ibm2 = IBMModel2(parallelSentences[:300], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAlignment(parallelSentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugarit.render(parallelSentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugarit.render(parallelSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAlignment(parallelSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = em_ibm2.translation_table['geist']\n",
    "sorted(d.items(), key=lambda x: x[1],reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = em_ibm1.translation_table['geist']\n",
    "sorted(d.items(), key=lambda x: x[1],reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = em_ibm2.translation_table['licht']\n",
    "sorted(d.items(), key=lambda x: x[1],reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = em_ibm1.translation_table['licht']\n",
    "sorted(d.items(), key=lambda x: x[1],reverse=True)[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
